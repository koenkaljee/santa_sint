The three python scripts allow to perform scraping of Titter tweets. The main script is designed to store the retrieved tweets into a Postgres database, but different solutions may be adopted, e.g. storing the tweets to a MongoDB database or to a JSON file. It is needed to customize this script though in order to employ these alternatives solutions. Theoretically, this process should be limitless, unlike the APIs, there are no restrictions on the number of tweets to be retrieved and the temporal period in the past to perform the search. Practically, some limitations may be encountered which makes this process a bit less smooth; nonetheless, it allows to retrieve a reasonable amount of tweets in a reasonable amount of time, also depending on the computational power at disposal. Few virtual machines would be ideal, but even with one pc it is possible to retrieve a good amount of tweets. One limitation I have experienced is, after performing a great amount of scraping, to not be allowed to perform queries before the past 10 days. I have not certainty yet about the entity of such limitation, if it is a form of block from Twitter or not, but one workaround is to use a different IP address in case this scenario occurs. Another limitation, is that the scraping process may be killed after retrieving several tweets at once (e.g. 100K). In this case, it may be needed to manually stop the script before and store only the retrieved tweets.
There are two strategies to perform the scraping:
1) scraping the tweets all (or as many as needed) at once, and then writing to the database. This solution is more risky, because in case of any error all the tweets retrieved are lost. Normally though, it works. It should be noted that sometimes less tweets are stored then the ones retrieved.
2) scraping batches of tweets and storing them in the database. I have not tested this solution thoroughly, but it should work.




The python scripts for performing the scraping are:
- tweet.py: it defines some elements of a tweet to scrape based on the corresponding HTML tag. If any of the elements is not needed it is just possible to delete them; on the other hand, if more elements are needed, it is left as further investigation. The python library used for the parsing of the HTML elements is BeautifulSoup.
- query.py: it is the true heart of the scraping. Twitter employs the infinite scroll trick, that is every time the user scrolls to the bottom of the page, another batch of around 20 tweets are retrieved. Thus, it is not possible to retrieve all tweets in a page at once, but there needs to be a mechanism to scroll down the page continuosly to retrieve as many tweets as possible. What this code basically does, is to make an HTTP request passing the query string to Twitter. Based on the first and last tweet ids, it is possible to make another request containing these information to retrieve the next batch of tweets. This mechanism is employed over and over again. The functions in this script are passed the query string and an optional limit number representing the maximum number of tweets to be retrieved. Once the limit is reached, the script returns the number of tweets. This parameter is optional, thus if no limit is passed, the script will go on to fetch tweets until the user manually stops the process (ctrl+C twice) or there are no more tweets to be retrieved. In case of storing batches of tweets in the database before waiting for the program termination, the "position" of the last tweet retrieved is passed, allowing thus to continue retrieving the tweets from where it was last interrupted.
- main.py: this script is the executor, it passes the query string and optionally other parameters to the query script and stores in the database.